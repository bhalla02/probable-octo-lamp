{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede21209-7281-4ef1-a579-ed15fbc8a5ab",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "A linear Support Vector Machine (SVM) aims to find the optimal hyperplane that separates data points of different classes. For a linear SVM, the decision function can be written as:\n",
    "\n",
    "𝑓\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "=\n",
    "𝑤\n",
    "𝑇\n",
    "𝑥\n",
    "+\n",
    "𝑏\n",
    "f(x)=w \n",
    "T\n",
    " x+b\n",
    "\n",
    "where:\n",
    "\n",
    "𝑤\n",
    "w is the weight vector.\n",
    "𝑥\n",
    "x is the input feature vector.\n",
    "𝑏\n",
    "b is the bias term.\n",
    "\n",
    "\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "The objective function of a linear SVM is to maximize the margin between the two classes while minimizing classification errors. This can be formulated as a convex optimization problem:\n",
    "\n",
    "min\n",
    "⁡\n",
    "𝑤\n",
    ",\n",
    "𝑏\n",
    "1\n",
    "2\n",
    "∥\n",
    "𝑤\n",
    "∥\n",
    "2\n",
    "+\n",
    "𝐶\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "𝜉\n",
    "𝑖\n",
    "min \n",
    "w,b\n",
    "​\n",
    "  \n",
    "2\n",
    "1\n",
    "​\n",
    " ∥w∥ \n",
    "2\n",
    " +C∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ξ \n",
    "i\n",
    "​\n",
    " \n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "𝑦\n",
    "𝑖\n",
    "(\n",
    "𝑤\n",
    "𝑇\n",
    "𝑥\n",
    "𝑖\n",
    "+\n",
    "𝑏\n",
    ")\n",
    "≥\n",
    "1\n",
    "−\n",
    "𝜉\n",
    "𝑖\n",
    "∀\n",
    "𝑖\n",
    "y \n",
    "i\n",
    "​\n",
    " (w \n",
    "T\n",
    " x \n",
    "i\n",
    "​\n",
    " +b)≥1−ξ \n",
    "i\n",
    "​\n",
    " ∀i\n",
    "𝜉\n",
    "𝑖\n",
    "≥\n",
    "0\n",
    "∀\n",
    "𝑖\n",
    "ξ \n",
    "i\n",
    "​\n",
    " ≥0∀i\n",
    "\n",
    "where:\n",
    "\n",
    "∥\n",
    "𝑤\n",
    "∥\n",
    "2\n",
    "∥w∥ \n",
    "2\n",
    "  is the regularization term that tries to maximize the margin.\n",
    "𝐶\n",
    "C is the regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n",
    "𝜉\n",
    "𝑖\n",
    "ξ \n",
    "i\n",
    "​\n",
    "  are slack variables that allow some misclassification for non-linearly separable data.\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "The kernel trick allows SVMs to efficiently perform non-linear classification by mapping the input features into a higher-dimensional space where a linear separator can be found. The kernel function \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " ) computes the inner product in this high-dimensional space without explicitly computing the coordinates in that space.\n",
    "\n",
    "Common kernel functions include:\n",
    "\n",
    "Linear kernel: \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "𝑥\n",
    "𝑖\n",
    "𝑇\n",
    "𝑥\n",
    "𝑗\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=x \n",
    "i\n",
    "T\n",
    "​\n",
    " x \n",
    "j\n",
    "​\n",
    " \n",
    "Polynomial kernel: \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    "𝑇\n",
    "𝑥\n",
    "𝑗\n",
    "+\n",
    "𝑐\n",
    ")\n",
    "𝑑\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=(x \n",
    "i\n",
    "T\n",
    "​\n",
    " x \n",
    "j\n",
    "​\n",
    " +c) \n",
    "d\n",
    " \n",
    "Radial basis function (RBF) kernel: \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "exp\n",
    "⁡\n",
    "(\n",
    "−\n",
    "𝛾\n",
    "∥\n",
    "𝑥\n",
    "𝑖\n",
    "−\n",
    "𝑥\n",
    "𝑗\n",
    "∥\n",
    "2\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=exp(−γ∥x \n",
    "i\n",
    "​\n",
    " −x \n",
    "j\n",
    "​\n",
    " ∥ \n",
    "2\n",
    " )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q4. What is the role of support vectors in SVM? Explain with example.\n",
    "Support vectors are the data points that are closest to the separating hyperplane. These points are crucial because they define the position and orientation of the hyperplane. The SVM algorithm seeks to find the hyperplane that maximizes the margin between the support vectors of the two classes.\n",
    "\n",
    "Example:\n",
    "Consider a 2D dataset where we have two classes that are linearly separable. The SVM will find a line (hyperplane) that separates these classes with the maximum margin. The data points that lie on the edge of this margin are the support vectors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM\n",
    "To illustrate these concepts, we'll use graphs:\n",
    "\n",
    "Hyperplane: A line in 2D (or plane in higher dimensions) that separates the data points of different classes.\n",
    "Marginal plane: The planes parallel to the hyperplane that pass through the support vectors.\n",
    "Hard margin: No data points are allowed inside the margin. This is suitable for linearly separable data.\n",
    "Soft margin: Some data points are allowed inside the margin to account for non-linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fefc7b3d-8290-4df7-a83a-94a265f17687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70, 2), (30, 2), (70,), (30,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# We will only use the first two classes for binary classification\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f6385-978a-4679-99b3-a23bbfc85f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
